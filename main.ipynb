{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pixqc/einsum-puzzles/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to pixqc/einsum-puzzles!\n",
    "\n",
    "# Einsum (short for Einstein summation) is a powerful tool for tensor multiplication, summation, and permutation.\n",
    "# Multi-step tensor operations can be concisely expressed with a single einsum subscript.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a, b = np.random.randn(2, 3, 4), np.random.randn(2, 3, 4)\n",
    "out1 = np.transpose(np.tensordot(a, b, axes=([2], [2])), (0, 1, 3, 2))\n",
    "out2 = np.einsum(\"ijm,lkm->ijkl\", a, b)\n",
    "print(np.allclose(out1, out2))\n",
    "# ijm,lkm->ijkl... what? This notebook attempts to demystify einsum.\n",
    "\n",
    "# We'll start with multiplying two vectors,\n",
    "# we'll end with implementing multi-head attention.\n",
    "\n",
    "# Einsum is available on numpy, torch, jax, mlx, and tinygrad.\n",
    "# We'll use numpy, but the core idea applies to all tensor library.\n",
    "\n",
    "# (Einsum cheatsheet available at the end of this notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "\n",
    "# In einsum-land, we deal with tensor shapes.\n",
    "# We tell einsum what the output should look like,\n",
    "# and it does the operations for us.\n",
    "# np.einsum(\"INPUT->OUTPUT\", a)\n",
    "#            │      │\n",
    "#            │      └─── desired output shape\n",
    "#            └────────── a.shape\n",
    "\n",
    "# We use 'ijk' to represent dimensions.\n",
    "# When our input has one dimension,\n",
    "# we use one letter to represent that dimension.\n",
    "a = np.array([1, 2, 3])\n",
    "out = np.einsum(\"i->i\", a)  # identity function\n",
    "#                │  │\n",
    "#                │  └─── output: (3,)\n",
    "#                └────── a.shape = (3,); i = 3\n",
    "# 'i->i' means \"take the input and keep it the same\"\n",
    "print(np.allclose(a, out))\n",
    "\n",
    "# When our input has two dimensions,\n",
    "# we use two letters to represent each dimension.\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "out = np.einsum(\"ij->ij\", a)  # identity function\n",
    "#                │   │\n",
    "#                │   └─── output: (2, 3)\n",
    "#                └─────── a.shape = (2, 3); i=2, j=3\n",
    "print(np.allclose(a, out))\n",
    "\n",
    "# Likewise with three dims.\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"ijk->ijk\", a)  # identity function\n",
    "#                │    │\n",
    "#                │    └─── output: (2, 3, 4)\n",
    "#                └──────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
    "print(np.allclose(a, out))\n",
    "\n",
    "# Notes:\n",
    "# - We can use any letter, not only 'ijk'.\n",
    "#   Ie. np.einsum('abc->abc', a) is valid.\n",
    "# - The arrow and output subscript can be ommited.\n",
    "#   Ie. np.einsum('ijk', a) is valid.\n",
    "# - This 'ijk->ijk' thing is sometimes called subscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #1\n",
    "# Compute identity of vector a.\n",
    "a = np.arange(10)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(a, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #2\n",
    "# Compute identity of tensor a.\n",
    "a = np.arange(120).reshape(5, 4, 3, 2)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(a, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication\n",
    "\n",
    "# We separate multiple inputs with comma.\n",
    "# np.einsum(\"INPUT1,INPUT2->OUTPUT\", a, b)\n",
    "#            │      │       │\n",
    "#            │      │       └─── desired output shape\n",
    "#            │      └─────────── b.shape\n",
    "#            └────────────────── a.shape\n",
    "\n",
    "# When we repeat indices in the input,\n",
    "# einsum elementwise-multiplies along that dimension.\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "out = np.einsum(\"i,i->i\", a, b)\n",
    "#                │ │  │\n",
    "#                │ │  └──── output: (3,)\n",
    "#                │ │        \"keep original shape\"\n",
    "#                │ │\n",
    "#                │ └─────── b.shape = (3,); i = 3\n",
    "#                ├───────── a.shape = (3,); i = 3\n",
    "#                │\n",
    "#                ├───────── 'i' appears twice before the arrow\n",
    "#                └───────── they are elementwise multiplied\n",
    "print(np.allclose(a * b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #3\n",
    "# Multiply vector a and b.\n",
    "a = np.arange(10)\n",
    "b = np.arange(10)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a * b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #4\n",
    "# Multiply matrix a and b.\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "b = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a * b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summation\n",
    "\n",
    "# When we omit an index in the output,\n",
    "# we're telling einsum to sum over that dimension.\n",
    "a = np.array([1, 2, 3])\n",
    "out = np.einsum(\"i->\", a)\n",
    "#                │  │\n",
    "#                │  └─── output: scalar (all dims summed)\n",
    "#                └────── a.shape = (3,); i = 3\n",
    "print(np.allclose(np.sum(a), out))\n",
    "\n",
    "# 'ijk->' will reduce a 3d tensor into a scalar.\n",
    "a = np.arange(24).reshape(4, 3, 2)\n",
    "out = np.einsum(\"ijk->\", a)\n",
    "#                │    │\n",
    "#                │    └─── output: scalar (all dims summed)\n",
    "#                └────── a.shape = (3,); i = 3\n",
    "print(np.allclose(np.sum(a), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #5\n",
    "# Compute sum of vector a.\n",
    "a = np.arange(10)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(np.sum(a), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #6\n",
    "# Multiply vector a and b.\n",
    "a = np.arange(10)\n",
    "b = np.arange(10)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a * b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #7\n",
    "# Compute dot product of vector a and b.\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.sum(a * b), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our understanding\n",
    "\n",
    "# Q1: 'i->i produces identity, 'i,i->i' multiplies. Why?\n",
    "# Q2: What will 'ij->ji' do to a matrix?\n",
    "# Q3: Is 'ijk' a valid input for a 2d matrix?\n",
    "# Q4: Can we elementwise add with einsum?\n",
    "\n",
    "# (Answers are available at the end of this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over axis\n",
    "\n",
    "# Einsum output:\n",
    "# Whatever's gone is summed over,\n",
    "# whatever's left is kept.\n",
    "\n",
    "# What we learned we can do:\n",
    "# Omit an index from the output to sum over it.\n",
    "# What we can also do:\n",
    "# Keep an index to preserve that dimension.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"ij->i\", a)\n",
    "#                │   │\n",
    "#                │   └─── output: (2,)\n",
    "#                │        \"sum over j, keep i\"\n",
    "#                │\n",
    "#                └─────── a.shape = (2, 3); i=2, j=3\n",
    "print(np.allclose(np.sum(a, axis=1), out))\n",
    "\n",
    "# Same thing but on axis=0\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"ij->j\", a)\n",
    "#                │   │\n",
    "#                │   └─── output: (3,)\n",
    "#                │        \"sum over i, keep j\"\n",
    "#                │\n",
    "#                └─────── a.shape = (2, 3); i=2, j=3\n",
    "print(np.allclose(np.sum(a, axis=0), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #8\n",
    "# Compute np.sum(a, axis=1)\n",
    "a = np.arange(10).reshape(2, 5)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(np.sum(a, axis=1), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #9\n",
    "# Compute np.sum(a*b, axis=0)\n",
    "a = np.arange(10).reshape(2, 5)\n",
    "b = np.arange(10).reshape(2, 5)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.sum(a * b, axis=0), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #10\n",
    "# Compute np.sum(a, axis=1)\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(np.sum(a, axis=1), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal and trace\n",
    "\n",
    "# Diagonal: we use the same index for both dimensions.\n",
    "a = np.arange(9).reshape(3, 3)\n",
    "out = np.einsum(\"ii->i\", a)\n",
    "#                │   │\n",
    "#                │   └─── output: (3,)\n",
    "#                └─────── a.shape = (3, 3); i=3\n",
    "# 'ii' is saying: \"get items where row and column is the same\".\n",
    "print(np.allclose(np.diag(a), out))\n",
    "\n",
    "# Trace: it's diagonal but the output is summed.\n",
    "a = np.arange(9).reshape(3, 3)\n",
    "out = np.einsum(\"ii->\", a)\n",
    "#                │   │\n",
    "#                │   └─── output: scalar\n",
    "#                └─────── a.shape = (3, 3); i=3\n",
    "print(np.allclose(np.trace(a), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #11\n",
    "# Compute np.diag(a)\n",
    "a = np.arange(9).reshape(3, 3)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(np.diag(a), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #12\n",
    "# Compute np.trace(a)\n",
    "a = np.arange(9).reshape(3, 3)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(np.trace(a), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "\n",
    "# We can use einsum to broadcast.\n",
    "\n",
    "# Let’s do a quick refresher on tensor broadcasting:\n",
    "# - Right-align shapes, prepend 1s if needed.\n",
    "# - Dims are compatible if equal or 1.\n",
    "# - 1s are broadcast to match the other size.\n",
    "# - Result shape is max size for each dimension.\n",
    "\n",
    "# Broadcasting along an axis\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = np.arange(3)\n",
    "out = np.einsum(\"ij,j->ij\", a, b)\n",
    "#                │  │  │\n",
    "#                │  │  └─── output: (2, 3)\n",
    "#                │  │       'i' and 'j' are kept\n",
    "#                │  │\n",
    "#                │  └────── b.shape = (3,); j=3\n",
    "#                └───────── a.shape = (2, 3); i=2, j=3\n",
    "# One way to look at it:\n",
    "# 'ij,j->ij' is broadcasted to 'ij,ij->ij' by einsum.\n",
    "print(np.allclose(a * b, out))\n",
    "\n",
    "# Broadcasting with higher dimensions\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "b = np.arange(3)\n",
    "out = np.einsum(\"ijk,j->ijk\", a, b)\n",
    "#                │   │     │\n",
    "#                │   │     └─── output: (2, 3, 4)\n",
    "#                │   │          'i', 'j', and 'k' are kept\n",
    "#                │   │\n",
    "#                │   └───────── b.shape = (3, 1); j=3, ...=(1,)\n",
    "#                └─────────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
    "# Likewise: 'ijk,j->ijk' is broadcasted to 'ijk,ijk->ijk'.\n",
    "print(np.allclose(a * b[:, None], out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #13\n",
    "# Compute np.sum(a*b,axis=1)\n",
    "# b must be broadcasted.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = np.arange(3)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.sum(a * b, axis=1), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #14\n",
    "# Compute np.allclose(np.sum(a*b, axis=(1, 2))\n",
    "# b must be broadcasted.\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "b = np.arange(12).reshape(3, 4)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.sum(a * b, axis=(1, 2)), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer product\n",
    "\n",
    "# When we use different indices for each input,\n",
    "# and combine them all in the output,\n",
    "# we're telling einsum to compute the outer product.\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6, 7])\n",
    "out = np.einsum(\"i,j->ij\", a, b)\n",
    "#                │ │  │\n",
    "#                │ │  │\n",
    "#                │ │  └──── output: (3, 4)\n",
    "#                │ │        we create all combinations of i and j\n",
    "#                │ │\n",
    "#                │ └─────── b.shape = (4,); j = 4\n",
    "#                └───────── a.shape = (3,); i = 3\n",
    "print(np.allclose(np.outer(a, b), out))\n",
    "\n",
    "# This creates a 4d array by combining all elements of both input.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = np.arange(8).reshape(2, 4)\n",
    "out = np.einsum(\"ij,kl->ijkl\", a, b)\n",
    "#                │  │   │\n",
    "#                │  │   │\n",
    "#                │  │   └── output: (2, 3, 2, 4)\n",
    "#                │  │       we create all combinations of i, j, k, l\n",
    "#                │  │\n",
    "#                │  └────── b.shape = (2, 4); k = 2, l = 4\n",
    "#                └───────── a.shape = (2, 3); i = 2, j = 3\n",
    "print(np.allclose(np.outer(a, b).reshape(2, 3, 2, 4), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #15\n",
    "# Compute the outer product of vectors a and b\n",
    "a = np.arange(3)\n",
    "b = np.arange(4)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.outer(a, b), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #16\n",
    "# Compute the outer product of vector a and matrix b.\n",
    "a = np.arange(3)\n",
    "b = np.arange(4).reshape(2, 2)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.outer(a, b.flatten()).reshape(3, 2, 2), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposition and permutation\n",
    "\n",
    "# This is where einsum shines.\n",
    "# Einsum permutations are self-documenting.\n",
    "\n",
    "# Transpose: we swap the order of indices in the output.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"ij->ji\", a)\n",
    "#                │   │\n",
    "#                │   └─── output: (3, 2)\n",
    "#                │        we swap i and j\n",
    "#                │\n",
    "#                └─────── a.shape = (2, 3); i=2, j=3\n",
    "print(np.allclose(a.T, out))\n",
    "\n",
    "# Permute: we can rearrange dimensions in any order we want.\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"ijk->kji\", a)\n",
    "#                │    │\n",
    "#                │    └─── output: (4, 3, 2)\n",
    "#                │         we reverse the order of indices\n",
    "#                │\n",
    "#                └─────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
    "print(np.allclose(np.transpose(a, (2, 1, 0)), out))\n",
    "\n",
    "# Even with higher dimensions, it's clear what we're doing.\n",
    "a = np.arange(120).reshape(2, 3, 4, 5)\n",
    "out = np.einsum(\"ijkl->jilk\", a)\n",
    "#                │     │\n",
    "#                │     └─── output: (3, 2, 5, 4)\n",
    "#                │          we swap i-j and k-l\n",
    "#                │\n",
    "#                └───────── a.shape = (2, 3, 4, 5); i=2, j=3, k=4, l=5\n",
    "print(np.allclose(np.transpose(a, (1, 0, 3, 2)), out))\n",
    "\n",
    "# We can multiply elementwise then transpose.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"ij,ij->ji\", a, b)\n",
    "#                │  │   │\n",
    "#                │  │   └─── output: (3, 2)\n",
    "#                │  │        we multiply elementwise, then transpose\n",
    "#                │  │\n",
    "#                │  └─────── b.shape = (2, 3); i=2, j=3\n",
    "#                └────────── a.shape = (2, 3); i=2, j=3\n",
    "print(np.allclose((a * b).T, out))\n",
    "\n",
    "# And even permute.\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "b = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"ijk,ijk->kji\", a, b)\n",
    "#                │   │     │\n",
    "#                │   │     └─ output: (4, 3, 2)\n",
    "#                │   │        we multiply elementwise, then rearrange\n",
    "#                │   │\n",
    "#                │   └─────── b.shape = (2, 3, 4); i=2, j=3, k=4\n",
    "#                └─────────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
    "print(np.allclose((a * b).transpose(2, 1, 0), out))\n",
    "\n",
    "# Note: when multiplying and permuting,\n",
    "# the multiplication takes precedence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #17\n",
    "# Transpose the 2d matrix a.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(a.T, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #18\n",
    "# Permute the tensor from shape (2,3,4) to (4,2,3)\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "out.shape\n",
    "print(np.allclose(np.transpose(a, (2, 1, 0)), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #19\n",
    "# Permute the tensor from shape (2,3,4,5) to (5,3,2,4)\n",
    "a = np.arange(120).reshape(2, 3, 4, 5)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(np.transpose(a, (3, 1, 0, 2)), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #20\n",
    "# Multiply a and b elementwise, then permute to (4,2,3)\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "b = np.arange(24).reshape(2, 3, 4)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose((a * b).transpose(2, 0, 1), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "# Repeat on input to multiply, omit from ouptut to sum.\n",
    "# Combine both we get matrix multiplication.\n",
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"ij,jk->ik\", a, b)\n",
    "#                │  │   │\n",
    "#                │  │   ├──── output: (3, 3)\n",
    "#                │  │   │     i and k remain, j is summed over\n",
    "#                │  │   │\n",
    "#                │  └───┼──── second input: b.shape == (2, 3)\n",
    "#                │      │     j = 2; k = 3\n",
    "#                │      │\n",
    "#                ├──────┼──── first input: a.shape == (3, 2)\n",
    "#                │      │     i = 3; j = 2\n",
    "#                │      │\n",
    "#                ├──────┼──── 'j' appears twice on input\n",
    "#                └──────┼──── they are elementwise multiplied\n",
    "#                       │\n",
    "#                       ├──── 'j' is omitted from output\n",
    "#                       └──── they are summed over\n",
    "print(np.allclose(a @ b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #21\n",
    "# Compute a @ b\n",
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a @ b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #22\n",
    "# Compute (a @ b).T\n",
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose((a @ b).T, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #23\n",
    "# Compute a @ b.T\n",
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(6).reshape(3, 2)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a @ b.T, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #24\n",
    "# Compute a @ b\n",
    "a = np.arange(24).reshape(2, 3, 4)\n",
    "b = np.arange(20).reshape(4, 5)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a @ b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #25\n",
    "# Compute a @ b (batched matmul)\n",
    "a = np.random.randn(10, 3, 4)\n",
    "b = np.random.randn(10, 4, 5)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a @ b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #26\n",
    "# Compute np.tensordot(a,b,axes=([1],[0]))\n",
    "# The output should be (3, 4, 6)\n",
    "a = np.random.randn(3, 5, 4)\n",
    "b = np.random.randn(5, 6)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.tensordot(a, b, axes=([1], [0])), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #27\n",
    "# Compute np.tensordot(a,b,axes=([0],[1])).transpose(1,0,2)\n",
    "# The output should be (4, 3, 6)\n",
    "a = np.random.randn(5, 3, 4)\n",
    "b = np.random.randn(6, 5)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.tensordot(a, b, axes=([0], [1])).transpose(1, 0, 2), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #28\n",
    "# Compute np.tensordot(a,b,axes=([1],[0])).sum(axis=(1,2)).T\n",
    "# The output should be (6, 2)\n",
    "a = np.random.randn(2, 3, 4)\n",
    "b = np.random.randn(3, 5, 6)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(np.tensordot(a, b, axes=([1], [0])).sum(axis=(1, 2)).T, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our understanding\n",
    "\n",
    "# Q5: Let a=(3,2); b=(3,2), why is a@b.T expressed as 'ij,kj->ik'?\n",
    "# Q6: Let a=(3,2); b=(3,2), what's the difference between\n",
    "#     np.einsum('ij,jk->ik', a, b.T) and np.einsum('ij,kj->ik', a, b)?\n",
    "\n",
    "# (Answers are available at the end of this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrowless subscript\n",
    "\n",
    "# np.einsum('ijk,ijk->', a, b) is a valid einsum.\n",
    "# np.einsum('ijk,ijk', a, b) is also a valid einsum.\n",
    "\n",
    "# When we omit arrow and output subscript from einsum:\n",
    "# - Repeated indices across inputs are multiplied.\n",
    "# - Non-repeating indices remain in the output.\n",
    "# - For a single input, it acts as an identity operation.\n",
    "\n",
    "# Single input: the identity operation.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "out = np.einsum(\"ij\", a)\n",
    "#                │\n",
    "#                └─────── a.shape = (2, 3); i=2, j=3\n",
    "#                         no repeated indices, nothing to multiply\n",
    "#                         both 'i' and 'j' remain in output\n",
    "print(np.allclose(np.einsum(\"ij->ij\", a), out))\n",
    "\n",
    "# Multiple inputs, repeated indices.\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "out = np.einsum(\"i,i\", a, b)\n",
    "#                │ │\n",
    "#                │ └─────── b.shape = (3,); i=3\n",
    "#                └───────── a.shape = (3,); i=3\n",
    "#                           'i' repeated across inputs, so it's multiplied\n",
    "#                           no indices remain, so output is scalar\n",
    "print(np.allclose(np.einsum(\"i,i->\", a, b), out))\n",
    "\n",
    "# Matrix multiplication.\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = np.arange(6).reshape(3, 2)\n",
    "out = np.einsum(\"ij,jk\", a, b)\n",
    "#                │  │\n",
    "#                │  └─────── b.shape = (3, 2); j=3, k=2\n",
    "#                └────────── a.shape = (2, 3); i=2, j=3\n",
    "#                            'j' repeated across inputs, so it's multiplied\n",
    "#                            'i' and 'k' remain, forming output shape\n",
    "print(np.allclose(np.einsum(\"ij,jk->ik\", a, b), out))\n",
    "\n",
    "# Multiple inputs, no repeated indices: outer product.\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4, 5])\n",
    "out = np.einsum(\"i,j\", a, b)\n",
    "#                │ │\n",
    "#                │ └─────── b.shape = (3,); j=3\n",
    "#                └───────── a.shape = (2,); i=2\n",
    "#                           no repeated indices, no multiply\n",
    "#                           both 'i' and 'j' remain, forming output shape\n",
    "print(np.allclose(np.einsum(\"i,j->ij\", a, b), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ellipsis\n",
    "\n",
    "# We can use '...' to represent leftover dimension.\n",
    "# It's like a placeholder for indices.\n",
    "# Say we have 'ijkl', and we're only interested in 'kl'.\n",
    "# Doing '...kl' means the '...' represents 'ij'.\n",
    "\n",
    "# Transposing [:-2].\n",
    "a = np.random.randn(2, 3, 4, 5)\n",
    "out = np.einsum(\"...ij->...ji\", a)\n",
    "#                 │ │    │  │\n",
    "#                 │ │    │  └─── output: (2, 3, 5, 4)\n",
    "#                 │ │    │       we swap i and j at the end\n",
    "#                 │ │    └────── '...' = (2, 3)\n",
    "#                 │ │\n",
    "#                 │ └─────────── a.shape[-2:] = (4, 5); i=4, j=5\n",
    "#                 └───────────── a.shape[:-2] = '...'=(2, 3)\n",
    "print(np.allclose(a.transpose(0, 1, 3, 2), out))\n",
    "\n",
    "# Transposing head and tail.\n",
    "a = np.random.randn(2, 3, 4, 5)\n",
    "out = np.einsum(\"i...k->k...i\", a)\n",
    "#                │ │ │  │ │ │\n",
    "#                │ │ │  │ │ └─── output[0] = a.shape[3] = 5\n",
    "#                │ │ │  │ └───── '...' = (3, 4)\n",
    "#                │ │ │  └─────── output[3] = a.shape[0] = 2\n",
    "#                │ │ └────────── a.shape[-1] = 5\n",
    "#                │ └──────────── a.shape[1:3] = '...' = (3, 4)\n",
    "#                └────────────── a.shape[0] = 2\n",
    "# It's like saying:\n",
    "# We only care about 'ik' dim, the '...' represents (3,4).\n",
    "# When we use '...' on output, it means (3,4).\n",
    "print(np.allclose(a.transpose(3, 1, 2, 0), out))\n",
    "\n",
    "# Batched matmul.\n",
    "a = np.random.randn(2, 3, 4, 5)\n",
    "b = np.random.randn(5, 6)\n",
    "out = np.einsum(\"...j,jk->...k\", a, b)\n",
    "#                 │ │ │    │ │\n",
    "#                 │ │ │    │ └─── output: (2, 3, 4, 6); k=6\n",
    "#                 │ │ │    └───── '...' = (2, 3, 4)\n",
    "#                 │ │ │\n",
    "#                 │ │ └────────── b = (5, 6); j=5, k=6\n",
    "#                 │ └──────────── a[-1] = 5; j=5\n",
    "#                 └────────────── a[:-1] = (2, 3, 4); ...=(2, 3, 4)\n",
    "print(np.allclose(a @ b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #29\n",
    "# Transpose the last two dimension of tensor a.\n",
    "a = np.random.randn(2, 3, 4, 5, 6, 7)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
    "print(np.allclose(a.transpose(0, 1, 2, 3, 5, 4), out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puzzle #30\n",
    "a = np.random.randn(3, 4, 5)\n",
    "b = np.random.randn(5, 6)\n",
    "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
    "print(np.allclose(a @ b, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our understanding\n",
    "\n",
    "# Q7: What are the pros/cons of arrowless subscript?\n",
    "# Q8: When is ellipsis useful? When should we use it vs. not use it?\n",
    "\n",
    "# (Answers are available at the end of this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention... is all we need.\n",
    "# Let's implment an attention head with einsum!\n",
    "\n",
    "# B: batch size\n",
    "# L: sequence length\n",
    "# D: model dimension\n",
    "# H: number of attention heads in a layer\n",
    "# K: size of each attention key or value\n",
    "b, l, d, h, k = 16, 10, 32, 4, 8\n",
    "# In Karpathy's/llm.c's lingo: BLD = BTC\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "  e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "  return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "w_q_dk = np.random.randn(d, k)\n",
    "w_k_dk = np.random.randn(d, k)\n",
    "w_v_dk = np.random.randn(d, k)\n",
    "mask = np.where(np.tril(np.ones((l, l))) == 1, 0.0, -np.inf)\n",
    "\n",
    "\n",
    "def head_np(input_bld):\n",
    "  q_blk = input_bld @ w_q_dk\n",
    "  k_blk = input_bld @ w_k_dk\n",
    "  v_blk = input_bld @ w_v_dk\n",
    "  sqrt_k = np.sqrt(k_blk.shape[-1])\n",
    "  scores_bll = (q_blk @ k_blk.transpose(0, 2, 1)) / sqrt_k + mask\n",
    "  attention_weights_bll = softmax(scores_bll, axis=-1)\n",
    "  out_blk = attention_weights_bll @ v_blk\n",
    "  return out_blk\n",
    "\n",
    "\n",
    "# Puzzle #31\n",
    "def head(input_bld):\n",
    "  # <YOUR_ANSWER_HERE> implement a single attention head with einsum.\n",
    "  pass\n",
    "\n",
    "\n",
    "input_bld = np.random.randn(b, l, d)\n",
    "ho = head(input_bld)\n",
    "ho_np = head_np(input_bld)\n",
    "print(np.allclose(ho, ho_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention... is all we need... part 2.\n",
    "# Let's implment multi-head attention in einsum!\n",
    "\n",
    "\n",
    "# B: batch size\n",
    "# L: sequence length\n",
    "# D: model dimension\n",
    "# H: number of attention heads in a layer\n",
    "# K: size of each attention key or value\n",
    "b, l, d, h, k = 16, 10, 32, 4, 8\n",
    "# In Karpathy's/llm.c's lingo: BLD = BTC\n",
    "\n",
    "\n",
    "w_q_dhk = np.random.randn(d, h, k)\n",
    "w_k_dhk = np.random.randn(d, h, k)\n",
    "w_v_dhk = np.random.randn(d, h, k)\n",
    "w_o_hkd = np.random.randn(h, k, d)\n",
    "\n",
    "mask = np.where(np.tril(np.ones((l, l))) == 1, 0.0, -np.inf)\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "  e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "  return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def attention_np(input_bld):\n",
    "  q_blhk = np.dot(input_bld, w_q_dhk.reshape(d, h * k)).reshape(b, l, h, k)\n",
    "  k_blhk = np.dot(input_bld, w_k_dhk.reshape(d, h * k)).reshape(b, l, h, k)\n",
    "  v_blhk = np.dot(input_bld, w_v_dhk.reshape(d, h * k)).reshape(b, l, h, k)\n",
    "  q_bhlk = q_blhk.transpose(0, 2, 1, 3)\n",
    "  k_bhkl = k_blhk.transpose(0, 2, 3, 1)\n",
    "  v_bhlk = v_blhk.transpose(0, 2, 1, 3)\n",
    "  scores_bhll = np.matmul(q_bhlk, k_bhkl) / np.sqrt(k)\n",
    "  scores_bhll = softmax(scores_bhll + mask, axis=-1)\n",
    "  out_bhlk = np.matmul(scores_bhll, v_bhlk)\n",
    "  out_blhk = out_bhlk.transpose(0, 2, 1, 3)\n",
    "  out_bld = np.dot(out_blhk.reshape(b, l, h * k), w_o_hkd.reshape(h * k, d))\n",
    "  return out_bld\n",
    "\n",
    "\n",
    "# Puzzle #32\n",
    "def attention(input_bld):\n",
    "  # <YOUR_ANSWER_HERE> implement multi-head attention with einsum.\n",
    "  pass\n",
    "\n",
    "\n",
    "input_bld = np.random.randn(b, l, d)\n",
    "output_np = attention_np(input_bld)\n",
    "output_einsum = attention(input_bld)\n",
    "print(np.allclose(output_np, output_einsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our understanding: answered\n",
    "\n",
    "# Q1A: 'i,i->i' has repeated 'i' on inputs, they're multiplied.\n",
    "#      'i->i' has no repeated inputs.\n",
    "# Q2A: 'ij->ji' will transpose the matrix.\n",
    "# Q3A: 'ijk' is not a valid subscript against 2d matrix.\n",
    "# Q4A: no, the only elementwise operation supported by einsum is multiply.\n",
    "# Q5A: 'ij,kj->ik' performs a@b.T because the tensor being dotted over\n",
    "#      is at shape[1] for both matrix, and they're denoted by 'j'. It doens't\n",
    "#      matter where the mul+sum happens. 'ik' remains after matmul.\n",
    "# Q6A: np.einsum('ij,jk->ik', a, b.T) transposes b before doing matmul.\n",
    "#      np.einsum('ij,kj->ik', a, b) handles the transposition for us.\n",
    "# Q7A: pros of arrowless subscript: shorter; cons: can't transpose.\n",
    "# Q8A: ellipsis can be useful when working with batches of high-dim tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einsum cheatsheet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compare(einsum_expr, np_fn, *xs):\n",
    "  out = np.einsum(einsum_expr, *xs)\n",
    "  out_np = np_fn(*xs)\n",
    "  return np.allclose(out, out_np)\n",
    "\n",
    "\n",
    "# Vector operations\n",
    "xs = np.random.randn(3)\n",
    "compare(\"i\", lambda x: x, xs)\n",
    "compare(\"i->\", lambda x: np.sum(x), xs)\n",
    "\n",
    "# Matrix operations\n",
    "xs = np.random.randn(3, 3)\n",
    "compare(\"ij->ji\", lambda x: x.T, xs)\n",
    "compare(\"ij->\", lambda x: np.sum(x), xs)\n",
    "compare(\"ii->i\", lambda x: np.diag(x), xs)\n",
    "compare(\"ii->\", lambda x: np.trace(x), xs)\n",
    "compare(\"ij->j\", lambda x: np.sum(x, axis=0), xs)\n",
    "compare(\"ij->i\", lambda x: np.sum(x, axis=1), xs)\n",
    "\n",
    "# Elementwise operations with two vectors\n",
    "xs = np.random.randn(3), np.random.randn(3)\n",
    "compare(\"i,i->i\", lambda x1, x2: x1 * x2, *xs)\n",
    "compare(\"i,i->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
    "compare(\"i,j->ij\", lambda x1, x2: np.outer(x1, x2), *xs)\n",
    "compare(\"i,j->ji\", lambda x1, x2: np.outer(x1, x2).T, *xs)\n",
    "\n",
    "# Elementwise operations with two matrices\n",
    "xs = np.random.randn(2, 3, 4), np.random.randn(2, 3, 4)\n",
    "compare(\"ijk,ijk->ijk\", lambda x1, x2: x1 * x2, *xs)\n",
    "compare(\"ijk,ijk->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
    "compare(\"ijk,ijk->ij\", lambda x1, x2: np.sum(x1 * x2, axis=2), *xs)\n",
    "compare(\"ijk,ijk->k\", lambda x1, x2: np.sum(x1 * x2, axis=(0, 1)), *xs)\n",
    "\n",
    "# Broadcasting vector to matrix\n",
    "xs = np.random.randn(3, 4), np.random.randn(4)\n",
    "compare(\"ij,j->ij\", lambda x1, x2: x1 * x2[:, None].T, *xs)\n",
    "compare(\"ij,j->i\", lambda x1, x2: np.sum(x1 * x2, axis=1), *xs)\n",
    "\n",
    "# Broadcasting matrix to 3d tensor\n",
    "xs = np.random.randn(2, 3, 4), np.random.randn(3, 4)\n",
    "compare(\"ijk,jk->ijk\", lambda x1, x2: x1 * x2, *xs)\n",
    "compare(\"ijk,jk->ik\", lambda x1, x2: np.sum(x1 * x2, axis=1), *xs)\n",
    "\n",
    "# Basic ellipsis operations\n",
    "xs = np.random.randn(2, 3, 4, 5)\n",
    "compare(\"...ij->...ji\", lambda x: np.swapaxes(x, -2, -1), xs)\n",
    "compare(\"i...->...\", lambda x: np.sum(x, axis=0), xs)\n",
    "compare(\"ij...->...ij\", lambda x: np.moveaxis(x, [0, 1], [-2, -1]), xs)\n",
    "\n",
    "# Ellipsis with broadcasting\n",
    "xs = np.random.randn(2, 3, 4, 5), np.random.randn(4, 5)\n",
    "compare(\"...ij,ij->...i\", lambda x1, x2: np.sum(x1 * x2, axis=-1), *xs)\n",
    "\n",
    "# Element-wise operations and reductions\n",
    "xs = np.random.randn(3, 2), np.random.randn(3, 2)\n",
    "compare(\"ij,ij->ij\", lambda x1, x2: x1 * x2, *xs)\n",
    "compare(\"ij,ij->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
    "compare(\"ij,ij->j\", lambda x1, x2: np.sum(x1 * x2, axis=0), *xs)\n",
    "compare(\"ij,ij->i\", lambda x1, x2: np.sum(x1 * x2, axis=1), *xs)\n",
    "\n",
    "# Matrix multiplications\n",
    "xs = np.random.randn(3, 2), np.random.randn(3, 2)\n",
    "compare(\"ki,kj->ij\", lambda x1, x2: x1.T @ x2, *xs)\n",
    "compare(\"ik,jk->ij\", lambda x1, x2: x1 @ x2.T, *xs)\n",
    "compare(\"ik,jk->\", lambda x1, x2: np.sum(x1 @ x2.T), *xs)\n",
    "compare(\"ik,jk->j\", lambda x1, x2: np.sum(x1 @ x2.T, axis=0), *xs)\n",
    "compare(\"ik,jk->i\", lambda x1, x2: np.sum(x1 @ x2.T, axis=1), *xs)\n",
    "\n",
    "# Combining summation, multiplication, transposition\n",
    "xs = np.random.randn(2, 3, 4), np.random.randn(2, 3, 4)\n",
    "compare(\"ijk,ijk->ijk\", lambda x1, x2: x1 * x2, *xs)\n",
    "compare(\"ijk,ijk->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
    "compare(\"ijk,ijk->ij\", lambda x1, x2: np.sum(x1 * x2, axis=2), *xs)\n",
    "compare(\"ijk,ijk->k\", lambda x1, x2: np.sum(x1 * x2, axis=(0, 1)), *xs)\n",
    "compare(\"ikl,jkl->ij\", lambda x1, x2: np.tensordot(x1, x2, axes=([1, 2], [1, 2])), *xs)\n",
    "compare(\"ijm,lkm->\", lambda x1, x2: np.sum(np.tensordot(x1, x2, axes=([2], [2]))), *xs)\n",
    "compare(\"ikl,jkl->j\", lambda x1, x2: np.sum(np.tensordot(x1, x2, axes=([1, 2], [1, 2])),axis=0), *xs)  # fmt: skip\n",
    "compare(\"ijm,lkm->ijkl\", lambda x1, x2: np.transpose(np.tensordot(x1, x2, axes=([2], [2])), (0, 1, 3, 2)), *xs)  # fmt: skip\n",
    "compare(\"ijm,lkm->il\", lambda x1, x2: np.sum(np.transpose(np.tensordot(x1, x2, axes=([2], [2])), (0, 1, 3, 2)), axis=(1,2)), *xs)  # fmt: skip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
